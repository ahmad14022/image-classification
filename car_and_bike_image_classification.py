# -*- coding: utf-8 -*-
"""Car and Bike Image Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7OGs23DUV9unh5uRnLLVkPjXhqd9Cuo

#Car and Bike Image Classification


*   Ahmad Mushawwir -  6011615
*   Mabertha Berliana - 7117026
*   Muhammad Fida Raditya - 5895961
*   Raihan Ferdyanza - 5915464

Hal yang sudah dicoba dan disesuaikan pada Source Code
*   Split Folder Dataset
*   Mengubah Class Mode pada variabel train_generator dari 'binary' menjadi 'categorical' -> menghasilkan accuracy flat di 0.5000
*   pada ```model``` ubah  ```tf.keras.layers.Dense(1, activation='sigmoid')``` dari 'sigmoid' ke 'softmax' -> juga menghasilkan accuracy flat di 0.5000
*   Konfigurasi pada parameter-parameter ImageDataGenerator (rotasi gambar)
*   Ubah jumlah batch di ```train generator``` dari 64 -> 80 --> kecepatan training model meningkat
"""

import matplotlib.pyplot as plt
import tensorflow as tf
import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive/')

"""Kaggle API Credentials using Environment Variable - ChatGPT"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Pembekalan_Teknis_Stechoq_AI

import os
import shutil

# Kaggle API Credentials using Environment Variable - ChatGPT
# Path to the uploaded kaggle.json file in your Google Drive
drive_kaggle_path = 'kaggle.json'

# Destination path for the kaggle.json file within Colab environment
colab_kaggle_path = '/root/.kaggle/kaggle.json'

# Copy the file to the Colab environment
os.makedirs(os.path.dirname(colab_kaggle_path), exist_ok=True)
shutil.copy(drive_kaggle_path, colab_kaggle_path)

# Set the environment variables
os.environ['KAGGLE_CONFIG_DIR'] = os.path.dirname(colab_kaggle_path)

!kaggle datasets download -d utkarshsaxenadn/car-vs-bike-classification-dataset --force

# !unzip \*.zip && rm *.zip

"""#Split Folder"""

!pip install split-folders
import splitfolders

input_folder="Car-Bike-Dataset"
output_folder="dataset"

splitfolders.ratio(input_folder,output=output_folder,ratio=(.8,.1,.1),seed=42)

# Path ke direktori yang ingin Anda cek
directory_path = 'dataset'

# List semua file dalam direktori
file_list = os.listdir(directory_path)

# Cetak nama semua file
for file_name in file_list:
    print(file_name)

TRAINING_DIR = '/content/drive/MyDrive/Pembekalan_Teknis_Stechoq_AI/dataset/train'
VALIDATION_DIR = '/content/drive/MyDrive/Pembekalan_Teknis_Stechoq_AI/dataset/val'
TEST_DIR = '/content/drive/MyDrive/Pembekalan_Teknis_Stechoq_AI/dataset/test'

"""**Mendefinisikan Batasan Augmentasi Data**

Pada keras, perlu dilakukan pengaturan parameter untuk menentukan batasan dari proses augmentasi. Perlu diperhatikan bahwa proses augmentasi berfungsi untuk mengatasi masalah keterbatasan data latih namun harus tetap merepresentasikan data pada kasus nyata. Potongan kode di bawah digunakan untuk mendefinisikan objek data generator untuk data latih dan validasi. Beberapa parameter yang digunakan :  

1. rescale : mengubah nilai dari masing-masing cell sesuai dengan skala yang ditentukan, disini bilai dari cell menjadi skala 0 sampai 1
2. rotation_range : rentang nilai derajat rotasi
3. brigtness_range : rentang nilai acak kecerahan gambar yang divariasikan
4. zoom_range : rentang nilai zoom gambar
5. horizontal_flip : parameter yang menentukan apakah ada variasi gambar dibalik secara horizontal  
6. vertical_flip : parameter yang menentukan apakah ada variasi gambar dibalik secara vertical
7. validation_split : proporsi nilai validasi  

Untuk bacaan lebih lanjut : https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator
"""

#bisa divariasikan

train_datagen = ImageDataGenerator(
    rescale=1.0/255,
    rotation_range=40, #ubahan rotation dari 30 ke 40
    brightness_range=[0.5,1.0],
    zoom_range=0.4,
    horizontal_flip=True,
    vertical_flip=True,
    validation_split=0.2,
)

val_datagen = ImageDataGenerator(
    rescale=1.0/255,
)

train_generator = train_datagen.flow_from_directory(
    TRAINING_DIR,
    batch_size=80, # ubah batch size dari 64 ke 80
    class_mode='binary', #ubah class mode dari binary ke categorical --> hasil : tetep sama 2 kelas | yang binary acc: train=87 val 88
    target_size=(150,150)
)

val_generator = val_datagen.flow_from_directory(
    VALIDATION_DIR,
    batch_size=32,
    class_mode='binary', #ubah class mode dari binary ke categorical --> hasil : tetep sama 2 kelas | yang binary acc: train=87 val 88
    target_size=(150,150)
)

# backup cnn

# model = tf.keras.Sequential([
#     # coba eksplorasi layer di sini
#     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
#     tf.keras.layers.Conv2D(8, (3,3), activation='relu'),
#     tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
#     tf.keras.layers.Flatten(),
#     tf.keras.layers.Dense(128, activation='relu'),
#     tf.keras.layers.Dense(64, activation='relu'),
#     tf.keras.layers.Dense(1, activation='sigmoid')
# ])

model = tf.keras.Sequential([
    # coba eksplorasi layer di sini
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid') #ganti dari 'sigmoid' ke 'softmax'
])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.87 and logs.get('val_accuracy')>0.86):
      print("\nNilai akurasi telah mencapai kriteria!")
      self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.Adam(lr=1e-4)
model.compile(optimizer=optimizer,
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_generator,
              epochs=80,
              validation_data=val_generator,
              callbacks=[callbacks])

"""#Confusion Matrix Evaluation"""

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
test_result=model.predict()

plt.plot(history.history['loss'], color='r', label='Train')
plt.plot(history.history['val_loss'], color='b', label='Val')
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')
plt.show()

plt.plot(history.history['accuracy'], color='r', label='Train')
plt.plot(history.history['val_accuracy'], color='b', label='Val')
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')
plt.show()